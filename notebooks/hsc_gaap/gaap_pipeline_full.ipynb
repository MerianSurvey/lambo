{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaap pipeline \n",
    "\n",
    "Use this notebook to run `gaap` photometry on Merian-reduced data.\n",
    "\n",
    "Make sure that you are in the right environment! When activating the jupyter notebook:\n",
    "\n",
    "        module load anaconda3/2022.5\n",
    "        . /scratch/gpfs/am2907/Merian/gaap/lambo/scripts/setup_env_w40.sh\n",
    "        jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding default configuration file with /scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/share/eups/Linux64/dustmaps_cachedata/g41a3ec361e+ac198e9f13/config/.dustmapsrc\n"
     ]
    }
   ],
   "source": [
    "import lsst.daf.butler as dafButler\n",
    "import numpy as np\n",
    "import glob\n",
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getenv('LAMBO_HOME'), 'lambo/scripts/'))\n",
    "from hsc_gaap.deploy_gaap_array import deploy_training_job\n",
    "from hsc_gaap.check_gaap_run import checkRun\n",
    "from hsc_gaap.find_patches_to_reduce import * \n",
    "from hsc_gaap.compile_catalogs import compileCatalogs\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 1: What patches do we need to reduce?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to identify patches that have the necessary merian data products for `gaap` processing and have not already been processed. Patches need to have:\n",
    "\n",
    "- deepCoadd_ref\n",
    "- deepCoadd_meas\n",
    "- deepCoadd_scarletModelData\n",
    "- deepCoadd_calexp\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of all Merian tracts with reduced data, and we will search through them to see which patches fit our criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = '/scratch/gpfs/am2907/Merian/gaap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_collection = \"DECam/runs/merian/dr1_wide\"\n",
    "data_type = \"deepCoadd_calexp\"\n",
    "skymap = \"hsc_rings_v1\"\n",
    "butler = dafButler.Butler('/projects/MERIAN/repo/', collections=output_collection, skymap=skymap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "patches = np.array([[data_id['tract'], data_id[\"patch\"]] for data_id in butler.registry.queryDataIds (['tract','patch'], datasets=data_type, \n",
    "                                                 collections=output_collection, skymap=skymap)])\n",
    "patches = patches[patches[:, 0].argsort()]\n",
    "tracts, idx = np.unique(patches[:,0], return_index=True) \n",
    "patches_by_tract = np.split(patches[:,1] ,idx[1:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now find patches with necessary data products in N708:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285 patches in 285 tracts with necessary data products in N708\n"
     ]
    }
   ],
   "source": [
    "tracts_n708 = []\n",
    "patches_n708 = []\n",
    "for tract in tracts:\n",
    "    patches = findReducedPatches(tract)\n",
    "    if len(patches) > 0:\n",
    "        tracts_n708.append(tract)\n",
    "        patches_n708.append(patches)\n",
    "\n",
    "tracts_n708 = np.array(tracts_n708)\n",
    "print(f\"{sum([len(p) for p in patches_n708])} patches in {len(tracts_n708)} tracts with necessary data products in N708\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14319 patches in 285 tracts with necessary data products in N708\n"
     ]
    }
   ],
   "source": [
    "print(f\"{sum([len(p) for p in patches_n708])} patches in {len(tracts_n708)} tracts with necessary data products in N708\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in N540:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318 patches in 318 tracts with necessary data products in N540\n"
     ]
    }
   ],
   "source": [
    "tracts_n540 = []\n",
    "patches_n540 = []\n",
    "for tract in tracts:\n",
    "    patches = findReducedPatches(tract, band=\"N540\")\n",
    "    if len(patches) > 0:\n",
    "        tracts_n540.append(tract)\n",
    "        patches_n540.append(patches)\n",
    "\n",
    "tracts_n540 = np.array(tracts_n540)\n",
    "print(f\"{sum([len(p) for p in patches_n540])} patches in {len(tracts_n540)} tracts with necessary data products in N540\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15259 patches in 318 tracts with necessary data products in N540\n"
     ]
    }
   ],
   "source": [
    "print(f\"{sum([len(p) for p in patches_n540])} patches in {len(tracts_n540)} tracts with necessary data products in N540\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might be interested in seeing which patches have only one but not both bands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 355/355 [00:00<00:00, 19958.15it/s]\n"
     ]
    }
   ],
   "source": [
    "n708_non540_tracts = []\n",
    "n708_non540_patches = []\n",
    "n540_non708_tracts = []\n",
    "n540_non708_patches = []\n",
    "for tract in tqdm(tracts):\n",
    "    if tract in tracts_n540:\n",
    "        patches_n540_i = patches_n540[np.where(tracts_n540 == tract)[0][0]]\n",
    "    else:\n",
    "        patches_n540_i=[]\n",
    "    if tract in tracts_n708:\n",
    "        patches_n708_i = patches_n708[np.where(tracts_n708 == tract)[0][0]]\n",
    "    else:\n",
    "        patches_n708_i=[]\n",
    "        \n",
    "    n708_non540 = list(set(patches_n708_i) - set(patches_n540_i))\n",
    "    n540_non708 = list(set(patches_n540_i) - set(patches_n708_i))\n",
    "\n",
    "    if len(n708_non540)>0:\n",
    "        n708_non540_tracts.append(tract)\n",
    "        n708_non540_patches.append(n708_non540)\n",
    "\n",
    "    if len(n540_non708)>0:\n",
    "        n540_non708_tracts.append(tract)\n",
    "        n540_non708_patches.append(n540_non708)\n",
    "\n",
    "numpatches_n540_only = sum([len(p) for p in n540_non708_patches])\n",
    "numpatches_n708_only = sum([len(p) for p in n708_non540_patches])\n",
    "numpatches_total = sum([len(p) for p in patches_by_tract])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3831 patches in 115 tracts that have N540 and no N708 \n",
      "There are 2891 patches in 89 tracts that have N708 and no N540 \n",
      "There are 26690 with both N540 and N708\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {numpatches_n540_only} patches in {len(n540_non708_tracts)} tracts that have N540 and no N708 \")\n",
    "print(f\"There are {numpatches_n708_only} patches in {len(n708_non540_tracts)} tracts that have N708 and no N540 \")\n",
    "print(f'There are {numpatches_total - numpatches_n708_only - numpatches_n540_only} with both N540 and N708')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a csv with the info if you want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveMerianReducedPatchList(tracts, os.path.join(repo, \"reducedPatches_N708.csv\"))\n",
    "# saveMerianReducedPatchList(tracts, os.path.join(repo, \"reducedPatches_N540.csv\"), band=\"N540\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now find patches that haven't yet been `gaap` processed:\n",
    "\n",
    "- N708 here is all patches that have N708 (whether they have N540 also or not)\n",
    "- N540 is patches with only N540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 285/285 [00:28<00:00, 10.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3464 patches in 79 tracts to be reduced with N708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tracts_n708_nogaap = []\n",
    "patches_n708_nogaap = []\n",
    "for i, tract in enumerate(tqdm(tracts_n708)):\n",
    "    patches_mer  = patches_n708[i]\n",
    "    patches_gaap = findGaapReducedPatches(tract, repo=repo)\n",
    "    if len(set(patches_mer) - set(patches_gaap)) > 0:\n",
    "        tracts_n708_nogaap.append(tract)\n",
    "        patches_n708_nogaap.append(list(set(patches_mer) - set(patches_gaap)))\n",
    "\n",
    "\n",
    "numpatches_n708_nogaap = sum([len(p) for p in patches_n708_nogaap])\n",
    "print(f\"{numpatches_n708_nogaap} patches in {len(tracts_n708_nogaap)} tracts to be reduced with N708\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 115/115 [00:03<00:00, 30.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3831 patches in 115 tracts to be reduced with only N540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tracts_n540_nogaap = []\n",
    "patches_n540_nogaap = []\n",
    "for i, tract in enumerate(tqdm(n540_non708_tracts)):\n",
    "    patches_mer  = n540_non708_patches[i]\n",
    "    patches_gaap = findGaapReducedPatches(tract, repo=repo)\n",
    "    if len(set(patches_mer) - set(patches_gaap)) > 0:\n",
    "        tracts_n540_nogaap.append(tract)\n",
    "        patches_n540_nogaap.append(list(set(patches_mer) - set(patches_gaap)))\n",
    "\n",
    "numpatches_n540_nogaap = sum([len(p) for p in patches_n540_nogaap])\n",
    "print(f\"{numpatches_n540_nogaap} patches in {len(tracts_n540_nogaap)} tracts to be reduced with only N540\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts_notcompiled_N708 = [tract for tract in tracts_n708 if not hasCompiledCatalog(tract)]\n",
    "tracts_notcompiled_N540 = [tract for tract in tracts_n540 if not hasCompiledCatalog(tract)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved file to /scratch/gpfs/am2907/Merian/gaap/notGaapReduced_N540.csv.\n"
     ]
    }
   ],
   "source": [
    "# saveGaapReducedPatchList(tracts, os.path.join(repo, \"GaapReduced.csv\"))\n",
    "# saveGaapNotReducedPatchList(tracts, os.path.join(repo, \"notGaapReduced.csv\"), notionformat=False)\n",
    "saveGaapNotReducedPatchList(tracts_n540_nogaap, os.path.join(repo, \"notGaapReduced_N540.csv\"), patches = patches_n540_nogaap,\n",
    "                            notionformat=True, band=\"N540\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It will take ~ 121.6 hours to download HSC images for 7295 patches\n",
      "It will take ~ 4.4 TBs to download HSC images for 7295 patches\n",
      "Once the data has been downloaded, it will take ~ 182.4 hours to run gaap on 7295 patches\n",
      "It will take ~ 1.5 TBs to save the gaap catalogs for 7295 patches\n"
     ]
    }
   ],
   "source": [
    "npatches = numpatches_n540_nogaap + numpatches_n708_nogaap\n",
    "print(f\"It will take ~ {npatches/60:.1f} hours to download HSC images for {npatches} patches\")\n",
    "print(f\"It will take ~ {npatches*.6/1000:.1f} TBs to download HSC images for {npatches} patches\")\n",
    "print(f\"Once the data has been downloaded, it will take ~ {npatches/20/2:.1f} hours to run gaap on {npatches} patches\")\n",
    "print(f\"It will take ~ {npatches*.212/1000:.1f} TBs to save the gaap catalogs for {npatches} patches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRACT:8769, 2\n",
      "TRACT:9085, 3\n",
      "TRACT:9698, 9\n",
      "TRACT:9709, 77\n",
      "TRACT:9710, 76\n",
      "TRACT:9711, 76\n",
      "TRACT:9712, 42\n",
      "TRACT:9713, 42\n",
      "TRACT:9714, 14\n",
      "TRACT:9798, 13\n",
      "TRACT:9799, 74\n",
      "TRACT:9800, 81\n",
      "TRACT:9801, 81\n",
      "TRACT:9802, 81\n",
      "TRACT:9803, 81\n",
      "TRACT:9804, 81\n",
      "TRACT:9805, 81\n",
      "TRACT:9806, 81\n",
      "TRACT:9807, 81\n",
      "TRACT:9808, 80\n",
      "TRACT:9809, 79\n",
      "TRACT:9810, 79\n",
      "TRACT:9811, 75\n",
      "TRACT:9812, 4\n",
      "TRACT:9814, 2\n",
      "TRACT:9815, 81\n",
      "TRACT:9816, 80\n",
      "TRACT:9817, 80\n",
      "TRACT:9818, 80\n",
      "TRACT:9819, 81\n",
      "TRACT:9820, 54\n",
      "TRACT:9821, 2\n",
      "TRACT:9828, 37\n",
      "TRACT:9833, 81\n",
      "TRACT:9837, 69\n",
      "TRACT:9838, 64\n",
      "TRACT:9839, 5\n",
      "TRACT:9862, 21\n",
      "TRACT:9863, 9\n",
      "TRACT:9939, 4\n",
      "TRACT:9940, 50\n",
      "TRACT:9941, 81\n",
      "TRACT:9942, 81\n",
      "TRACT:9943, 81\n",
      "TRACT:9944, 64\n",
      "TRACT:9945, 2\n",
      "TRACT:9949, 9\n",
      "TRACT:9950, 41\n",
      "TRACT:9951, 41\n",
      "TRACT:9952, 41\n",
      "TRACT:9953, 41\n",
      "TRACT:10040, 5\n",
      "TRACT:10041, 65\n",
      "TRACT:10042, 81\n",
      "TRACT:10043, 81\n",
      "TRACT:10044, 79\n",
      "TRACT:10045, 79\n",
      "TRACT:10046, 81\n",
      "TRACT:10047, 81\n",
      "TRACT:10048, 81\n",
      "TRACT:10049, 69\n",
      "TRACT:10050, 62\n",
      "TRACT:10051, 79\n",
      "TRACT:10052, 80\n",
      "TRACT:10053, 78\n",
      "TRACT:10055, 4\n",
      "TRACT:10057, 80\n",
      "TRACT:10058, 80\n",
      "TRACT:10060, 81\n",
      "TRACT:10061, 81\n",
      "TRACT:10062, 50\n",
      "TRACT:10070, 2\n",
      "TRACT:10078, 4\n",
      "TRACT:10182, 6\n",
      "TRACT:10183, 34\n",
      "TRACT:10184, 59\n",
      "TRACT:10185, 81\n",
      "TRACT:10186, 27\n",
      "TRACT:10283, 27\n",
      "TRACT:10285, 42\n",
      "TRACT:10286, 43\n",
      "TRACT:10287, 41\n",
      "TRACT:10288, 36\n",
      "TRACT:10289, 36\n",
      "TRACT:10290, 36\n",
      "TRACT:10291, 19\n",
      "TRACT:10292, 11\n",
      "TRACT:10293, 28\n",
      "TRACT:10294, 26\n",
      "TRACT:10295, 26\n",
      "TRACT:10296, 26\n",
      "TRACT:10297, 24\n",
      "TRACT:10298, 23\n",
      "TRACT:10299, 19\n",
      "TRACT:10300, 18\n",
      "TRACT:10301, 18\n",
      "TRACT:10302, 17\n",
      "TRACT:10303, 14\n",
      "TRACT:10304, 9\n",
      "TRACT:10426, 15\n",
      "TRACT:10427, 33\n",
      "TRACT:10428, 2\n"
     ]
    }
   ],
   "source": [
    "for tract, npatch in zip(tracts_n708_nogaap, npatches):\n",
    "    if npatch > 0:\n",
    "        print (f'TRACT:{tract}, {npatch}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 2: Download the data\n",
    "\n",
    "We need to download the HSC data for all of the tracts we need to reduce. *Be warned, this takes a while and uses a lot of storage.*\n",
    "\n",
    "It is recommended to run the following in a bash screen because depending on how much data you need to download, it can take many hours.\n",
    "\n",
    "The following will download images for tract 9813 to `/scratch/gpfs/am2907/Merian/gaap/S20A/deepCoadd_calexp/9813` and the blendedness catalogs to `/scratch/gpfs/am2907/Merian/gaap/S20A/gaapTable/9813`:\n",
    "- Unless `--only_merian=False`, this will only download the patches that have been reduced by Merian.\n",
    "- You can download all of the Merian-reduced data in one go if you set `--alltracts=True`. Be careful with this, because it is ****lots**** of data!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    screen -L -S downloadtract    \n",
    "    \n",
    "    cd /scratch/gpfs/am2907/Merian/gaap\n",
    "    . lambo/scripts/setup_env_w40.sh\n",
    "    python3 lambo/scripts/hsc_gaap/download_S20A.py --tract=9813 --outdir=\"/scratch/gpfs/am2907/Merian/gaap/\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To exit screen do `ctrl a d` and to reattach do `screen -r downloaddata`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 3: Make slurm scripts and submit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write one slurm script for each tract – each of which is a job array with one job for each patch. \n",
    "You can submit the scripts as you write them if you want, but beware that there is an upper limit for the number of jobs you can submit at once to the queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10428"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracts_n708_nogaap[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 9986322\n"
     ]
    }
   ],
   "source": [
    "for tract in tracts_n708_nogaap[78:]:\n",
    "    deploy_training_job(tract, band = \"N708\", filter_jobs=5,\n",
    "                        python_file='lambo/scripts/hsc_gaap/run_gaap.py',\n",
    "                        name='gaap', email=\"am2907@princeton.edu\", outname = None, \n",
    "                        repo='/scratch/gpfs/am2907/Merian/gaap', scriptdir=\"/scratch/gpfs/am2907/Merian/gaap/\", \n",
    "                        submit=True, fixpatches=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: error: QOSMaxSubmitJobPerUserLimit\n",
      "sbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)\n",
      "sbatch: error: QOSMaxSubmitJobPerUserLimit\n",
      "sbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 9986326\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m tract \u001b[39min\u001b[39;00m tracts_n540_nogaap[:\u001b[39m5\u001b[39m]:\n\u001b[0;32m----> 2\u001b[0m     deploy_training_job(tract, band \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mN540\u001b[39;49m\u001b[39m\"\u001b[39;49m, filter_jobs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m                         python_file\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlambo/scripts/hsc_gaap/run_gaap.py\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m                         name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgaap\u001b[39;49m\u001b[39m'\u001b[39;49m, email\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mam2907@princeton.edu\u001b[39;49m\u001b[39m\"\u001b[39;49m, outname \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m, \n\u001b[1;32m      5\u001b[0m                         repo\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/scratch/gpfs/am2907/Merian/gaap\u001b[39;49m\u001b[39m'\u001b[39;49m, scriptdir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/scratch/gpfs/am2907/Merian/gaap/\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m      6\u001b[0m                         submit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, fixpatches\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m/scratch/gpfs/am2907/Merian/gaap/lambo/scripts/hsc_gaap/deploy_gaap_array.py:25\u001b[0m, in \u001b[0;36mdeploy_training_job\u001b[0;34m(tract, band, filter_jobs, python_file, name, email, outname, repo, scriptdir, submit, fixpatches)\u001b[0m\n\u001b[1;32m     23\u001b[0m patches_n708 \u001b[39m=\u001b[39m findReducedPatches(tract, band\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mN708\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[39mif\u001b[39;00m band\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mN540\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 25\u001b[0m     patches_n540 \u001b[39m=\u001b[39m findReducedPatches(tract, band\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mN540\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     26\u001b[0m     patches \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(patches_n540) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(patches_n708)))\n\u001b[1;32m     27\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/gpfs/am2907/Merian/gaap/lambo/scripts/hsc_gaap/gaap.py:575\u001b[0m, in \u001b[0;36mfindReducedPatches\u001b[0;34m(tract, band, output_collection, skymap)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfindReducedPatches\u001b[39m(tract, band\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mN708\u001b[39m\u001b[39m\"\u001b[39m, output_collection\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDECam/runs/merian/dr1_wide\u001b[39m\u001b[39m'\u001b[39m, skymap\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhsc_rings_v1\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    573\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Find all patches that have the necessary Merian data products for gaap photometry\"\"\"\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m     butler \u001b[39m=\u001b[39m dafButler\u001b[39m.\u001b[39;49mButler(\u001b[39m'\u001b[39;49m\u001b[39m/projects/MERIAN/repo/\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    577\u001b[0m     dataId \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(tract\u001b[39m=\u001b[39mtract, band\u001b[39m=\u001b[39mband, skymap\u001b[39m=\u001b[39mskymap)\n\u001b[1;32m    579\u001b[0m     deepCoadd_ref_patches \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m([item\u001b[39m.\u001b[39mdataId[\u001b[39m\"\u001b[39m\u001b[39mpatch\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m \n\u001b[1;32m    580\u001b[0m                                 butler\u001b[39m.\u001b[39mregistry\u001b[39m.\u001b[39mqueryDatasets(\u001b[39m'\u001b[39m\u001b[39mdeepCoadd_ref\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m    581\u001b[0m                                 dataId\u001b[39m=\u001b[39mdataId, collections\u001b[39m=\u001b[39moutput_collection,\n\u001b[1;32m    582\u001b[0m                                 skymap\u001b[39m=\u001b[39mskymap)])\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/share/eups/Linux64/daf_butler/g07e057c463+176e5d5642/python/lsst/daf/butler/_butler.py:279\u001b[0m, in \u001b[0;36mButler.__init__\u001b[0;34m(self, config, butler, collections, run, searchPaths, writeable, inferDefaults, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m writeable \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m     writeable \u001b[39m=\u001b[39m run \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregistry \u001b[39m=\u001b[39m Registry\u001b[39m.\u001b[39;49mfromConfig(\n\u001b[1;32m    280\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_config, butlerRoot\u001b[39m=\u001b[39;49mbutlerRoot, writeable\u001b[39m=\u001b[39;49mwriteable, defaults\u001b[39m=\u001b[39;49mdefaults\n\u001b[1;32m    281\u001b[0m )\n\u001b[1;32m    282\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatastore \u001b[39m=\u001b[39m Datastore\u001b[39m.\u001b[39mfromConfig(\n\u001b[1;32m    283\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_config, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregistry\u001b[39m.\u001b[39mgetDatastoreBridgeManager(), butlerRoot\u001b[39m=\u001b[39mbutlerRoot\n\u001b[1;32m    284\u001b[0m )\n\u001b[1;32m    285\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstorageClasses \u001b[39m=\u001b[39m StorageClassFactory()\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/share/eups/Linux64/daf_butler/g07e057c463+176e5d5642/python/lsst/daf/butler/registry/_registry.py:230\u001b[0m, in \u001b[0;36mRegistry.fromConfig\u001b[0;34m(cls, config, butlerRoot, writeable, defaults)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39m# The base class implementation should trampoline to the correct\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[39m# subclass. No implementation should ever use this implementation\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39m# directly. If no class is specified, default to the standard\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[39m# registry.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m registry_cls, registry_config \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mdetermineTrampoline(config)\n\u001b[0;32m--> 230\u001b[0m \u001b[39mreturn\u001b[39;00m registry_cls\u001b[39m.\u001b[39;49mfromConfig(config, butlerRoot, writeable, defaults)\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/share/eups/Linux64/daf_butler/g07e057c463+176e5d5642/python/lsst/daf/butler/registries/sql.py:195\u001b[0m, in \u001b[0;36mSqlRegistry.fromConfig\u001b[0;34m(cls, config, butlerRoot, writeable, defaults)\u001b[0m\n\u001b[1;32m    188\u001b[0m database \u001b[39m=\u001b[39m DatabaseClass\u001b[39m.\u001b[39mfromUri(\n\u001b[1;32m    189\u001b[0m     \u001b[39mstr\u001b[39m(config\u001b[39m.\u001b[39mconnectionString),\n\u001b[1;32m    190\u001b[0m     origin\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39morigin\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m),\n\u001b[1;32m    191\u001b[0m     namespace\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnamespace\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    192\u001b[0m     writeable\u001b[39m=\u001b[39mwriteable,\n\u001b[1;32m    193\u001b[0m )\n\u001b[1;32m    194\u001b[0m managerTypes \u001b[39m=\u001b[39m RegistryManagerTypes\u001b[39m.\u001b[39mfromConfig(config)\n\u001b[0;32m--> 195\u001b[0m managers \u001b[39m=\u001b[39m managerTypes\u001b[39m.\u001b[39;49mloadRepo(database)\n\u001b[1;32m    196\u001b[0m \u001b[39mif\u001b[39;00m defaults \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     defaults \u001b[39m=\u001b[39m RegistryDefaults()\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/share/eups/Linux64/daf_butler/g07e057c463+176e5d5642/python/lsst/daf/butler/registry/managers.py:208\u001b[0m, in \u001b[0;36mRegistryManagerTypes.loadRepo\u001b[0;34m(self, database)\u001b[0m\n\u001b[1;32m    206\u001b[0m universe \u001b[39m=\u001b[39m DimensionUniverse(dimensionConfig)\n\u001b[1;32m    207\u001b[0m \u001b[39mwith\u001b[39;00m database\u001b[39m.\u001b[39mdeclareStaticTables(create\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m context:\n\u001b[0;32m--> 208\u001b[0m     instances \u001b[39m=\u001b[39m RegistryManagerInstances\u001b[39m.\u001b[39;49minitialize(database, context, types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, universe\u001b[39m=\u001b[39;49muniverse)\n\u001b[1;32m    209\u001b[0m     versions \u001b[39m=\u001b[39m instances\u001b[39m.\u001b[39mgetVersions()\n\u001b[1;32m    210\u001b[0m \u001b[39m# verify that configured versions are compatible with schema\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/share/eups/Linux64/daf_butler/g07e057c463+176e5d5642/python/lsst/daf/butler/registry/managers.py:264\u001b[0m, in \u001b[0;36mRegistryManagerInstances.initialize\u001b[0;34m(cls, database, context, types, universe)\u001b[0m\n\u001b[1;32m    262\u001b[0m kwargs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m {}\n\u001b[1;32m    263\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mattributes\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m types\u001b[39m.\u001b[39mattributes\u001b[39m.\u001b[39minitialize(database, context)\n\u001b[0;32m--> 264\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mdimensions\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m types\u001b[39m.\u001b[39;49mdimensions\u001b[39m.\u001b[39;49minitialize(database, context, universe\u001b[39m=\u001b[39;49muniverse)\n\u001b[1;32m    265\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mcollections\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m types\u001b[39m.\u001b[39mcollections\u001b[39m.\u001b[39minitialize(\n\u001b[1;32m    266\u001b[0m     database,\n\u001b[1;32m    267\u001b[0m     context,\n\u001b[1;32m    268\u001b[0m     dimensions\u001b[39m=\u001b[39mkwargs[\u001b[39m\"\u001b[39m\u001b[39mdimensions\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    269\u001b[0m )\n\u001b[1;32m    270\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mdatasets\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m types\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39minitialize(\n\u001b[1;32m    271\u001b[0m     database,\n\u001b[1;32m    272\u001b[0m     context,\n\u001b[1;32m    273\u001b[0m     collections\u001b[39m=\u001b[39mkwargs[\u001b[39m\"\u001b[39m\u001b[39mcollections\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    274\u001b[0m     dimensions\u001b[39m=\u001b[39mkwargs[\u001b[39m\"\u001b[39m\u001b[39mdimensions\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    275\u001b[0m )\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/share/eups/Linux64/daf_butler/g07e057c463+176e5d5642/python/lsst/daf/butler/registry/dimensions/static.py:116\u001b[0m, in \u001b[0;36mStaticDimensionRecordStorageManager.initialize\u001b[0;34m(cls, db, context, universe)\u001b[0m\n\u001b[1;32m    114\u001b[0m spatial \u001b[39m=\u001b[39m NamedKeyDict[DatabaseTopologicalFamily, List[DatabaseDimensionRecordStorage]]()\n\u001b[1;32m    115\u001b[0m \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m universe\u001b[39m.\u001b[39mgetDatabaseElements():\n\u001b[0;32m--> 116\u001b[0m     elementStorage \u001b[39m=\u001b[39m element\u001b[39m.\u001b[39;49mmakeStorage(db, context\u001b[39m=\u001b[39;49mcontext, governors\u001b[39m=\u001b[39;49mgovernors)\n\u001b[1;32m    117\u001b[0m     records[element] \u001b[39m=\u001b[39m elementStorage\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m element\u001b[39m.\u001b[39mspatial \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/share/eups/Linux64/daf_butler/g07e057c463+176e5d5642/python/lsst/daf/butler/core/dimensions/_database.py:264\u001b[0m, in \u001b[0;36mDatabaseDimensionElement.makeStorage\u001b[0;34m(self, db, context, governors)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m doImportType(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_storage[\u001b[39m\"\u001b[39m\u001b[39mcls\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    263\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, DatabaseDimensionRecordStorage)\n\u001b[0;32m--> 264\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49minitialize(db, \u001b[39mself\u001b[39;49m, context\u001b[39m=\u001b[39;49mcontext, config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_storage, governors\u001b[39m=\u001b[39;49mgovernors)\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/share/eups/Linux64/daf_butler/g07e057c463+176e5d5642/python/lsst/daf/butler/registry/dimensions/table.py:134\u001b[0m, in \u001b[0;36mTableDimensionRecordStorage.initialize\u001b[0;34m(cls, db, element, context, config, governors)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mif\u001b[39;00m element\u001b[39m.\u001b[39mspatial \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     governor \u001b[39m=\u001b[39m governors[element\u001b[39m.\u001b[39mspatial\u001b[39m.\u001b[39mgovernor]\n\u001b[0;32m--> 134\u001b[0m     skyPixOverlap \u001b[39m=\u001b[39m _SkyPixOverlapStorage\u001b[39m.\u001b[39;49minitialize(\n\u001b[1;32m    135\u001b[0m         db,\n\u001b[1;32m    136\u001b[0m         element,\n\u001b[1;32m    137\u001b[0m         context\u001b[39m=\u001b[39;49mcontext,\n\u001b[1;32m    138\u001b[0m         governor\u001b[39m=\u001b[39;49mgovernor,\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    140\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(db, element, table\u001b[39m=\u001b[39mtable, skyPixOverlap\u001b[39m=\u001b[39mskyPixOverlap)\n\u001b[1;32m    142\u001b[0m     \u001b[39m# Whenever anyone inserts a new governor dimension value, we want\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[39m# to enable overlaps for that value between this element and\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[39m# commonSkyPix.\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/share/eups/Linux64/daf_butler/g07e057c463+176e5d5642/python/lsst/daf/butler/registry/dimensions/table.py:371\u001b[0m, in \u001b[0;36m_SkyPixOverlapStorage.initialize\u001b[0;34m(cls, db, element, context, governor)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     op \u001b[39m=\u001b[39m db\u001b[39m.\u001b[39mensureTableExists\n\u001b[0;32m--> 371\u001b[0m summaryTable \u001b[39m=\u001b[39m op(\n\u001b[1;32m    372\u001b[0m     \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_SUMMARY_TABLE_NAME_SPEC\u001b[39m.\u001b[39;49mformat(element\u001b[39m=\u001b[39;49melement),\n\u001b[1;32m    373\u001b[0m     \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_makeSummaryTableSpec(element),\n\u001b[1;32m    374\u001b[0m )\n\u001b[1;32m    375\u001b[0m overlapTable \u001b[39m=\u001b[39m op(\n\u001b[1;32m    376\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_OVERLAP_TABLE_NAME_SPEC\u001b[39m.\u001b[39mformat(element\u001b[39m=\u001b[39melement),\n\u001b[1;32m    377\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_makeOverlapTableSpec(element),\n\u001b[1;32m    378\u001b[0m )\n\u001b[1;32m    379\u001b[0m \u001b[39mreturn\u001b[39;00m _SkyPixOverlapStorage(\n\u001b[1;32m    380\u001b[0m     db, element, summaryTable\u001b[39m=\u001b[39msummaryTable, overlapTable\u001b[39m=\u001b[39moverlapTable, governor\u001b[39m=\u001b[39mgovernor\n\u001b[1;32m    381\u001b[0m )\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/share/eups/Linux64/daf_butler/g07e057c463+176e5d5642/python/lsst/daf/butler/registry/interfaces/_database.py:120\u001b[0m, in \u001b[0;36mStaticTablesContext.addTable\u001b[0;34m(self, name, spec)\u001b[0m\n\u001b[1;32m    117\u001b[0m name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_db\u001b[39m.\u001b[39m_mangleTableName(name)\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tableNames:\n\u001b[1;32m    119\u001b[0m     _checkExistingTableDefinition(\n\u001b[0;32m--> 120\u001b[0m         name, spec, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inspector\u001b[39m.\u001b[39;49mget_columns(name, schema\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_db\u001b[39m.\u001b[39;49mnamespace)\n\u001b[1;32m    121\u001b[0m     )\n\u001b[1;32m    122\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_db\u001b[39m.\u001b[39m_metadata\n\u001b[1;32m    123\u001b[0m \u001b[39massert\u001b[39;00m metadata \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mGuaranteed by context manager that returns this object.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/lib/python3.10/site-packages/sqlalchemy/engine/reflection.py:497\u001b[0m, in \u001b[0;36mInspector.get_columns\u001b[0;34m(self, table_name, schema, **kw)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return information about columns in `table_name`.\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[39mGiven a string `table_name` and an optional string `schema`, return\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    493\u001b[0m \n\u001b[1;32m    494\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_operation_context() \u001b[39mas\u001b[39;00m conn:\n\u001b[0;32m--> 497\u001b[0m     col_defs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdialect\u001b[39m.\u001b[39;49mget_columns(\n\u001b[1;32m    498\u001b[0m         conn, table_name, schema, info_cache\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfo_cache, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw\n\u001b[1;32m    499\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mfor\u001b[39;00m col_def \u001b[39min\u001b[39;00m col_defs:\n\u001b[1;32m    501\u001b[0m     \u001b[39m# make this easy and only return instances for coltype\u001b[39;00m\n\u001b[1;32m    502\u001b[0m     coltype \u001b[39m=\u001b[39m col_def[\u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36mget_columns\u001b[0;34m(self, connection, table_name, schema, **kw)\u001b[0m\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/lib/python3.10/site-packages/sqlalchemy/engine/reflection.py:55\u001b[0m, in \u001b[0;36mcache\u001b[0;34m(fn, self, con, *args, **kw)\u001b[0m\n\u001b[1;32m     53\u001b[0m ret \u001b[39m=\u001b[39m info_cache\u001b[39m.\u001b[39mget(key)\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     ret \u001b[39m=\u001b[39m fn(\u001b[39mself\u001b[39;49m, con, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m     56\u001b[0m     info_cache[key] \u001b[39m=\u001b[39m ret\n\u001b[1;32m     57\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/lib/python3.10/site-packages/sqlalchemy/dialects/postgresql/base.py:3859\u001b[0m, in \u001b[0;36mPGDialect.get_columns\u001b[0;34m(self, connection, table_name, schema, **kw)\u001b[0m\n\u001b[1;32m   3856\u001b[0m \u001b[39m@reflection\u001b[39m\u001b[39m.\u001b[39mcache\n\u001b[1;32m   3857\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_columns\u001b[39m(\u001b[39mself\u001b[39m, connection, table_name, schema\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m-> 3859\u001b[0m     table_oid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_table_oid(\n\u001b[1;32m   3860\u001b[0m         connection, table_name, schema, info_cache\u001b[39m=\u001b[39;49mkw\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39minfo_cache\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   3861\u001b[0m     )\n\u001b[1;32m   3863\u001b[0m     generated \u001b[39m=\u001b[39m (\n\u001b[1;32m   3864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39ma.attgenerated as generated\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3865\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mserver_version_info \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m12\u001b[39m,)\n\u001b[1;32m   3866\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mNULL as generated\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3867\u001b[0m     )\n\u001b[1;32m   3868\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mserver_version_info \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m10\u001b[39m,):\n\u001b[1;32m   3869\u001b[0m         \u001b[39m# a.attidentity != '' is required or it will reflect also\u001b[39;00m\n\u001b[1;32m   3870\u001b[0m         \u001b[39m# serial columns as identity.\u001b[39;00m\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36mget_table_oid\u001b[0;34m(self, connection, table_name, schema, **kw)\u001b[0m\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/lib/python3.10/site-packages/sqlalchemy/engine/reflection.py:55\u001b[0m, in \u001b[0;36mcache\u001b[0;34m(fn, self, con, *args, **kw)\u001b[0m\n\u001b[1;32m     53\u001b[0m ret \u001b[39m=\u001b[39m info_cache\u001b[39m.\u001b[39mget(key)\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     ret \u001b[39m=\u001b[39m fn(\u001b[39mself\u001b[39;49m, con, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m     56\u001b[0m     info_cache[key] \u001b[39m=\u001b[39m ret\n\u001b[1;32m     57\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/lib/python3.10/site-packages/sqlalchemy/dialects/postgresql/base.py:3736\u001b[0m, in \u001b[0;36mPGDialect.get_table_oid\u001b[0;34m(self, connection, table_name, schema, **kw)\u001b[0m\n\u001b[1;32m   3734\u001b[0m \u001b[39mif\u001b[39;00m schema:\n\u001b[1;32m   3735\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mbindparams(sql\u001b[39m.\u001b[39mbindparam(\u001b[39m\"\u001b[39m\u001b[39mschema\u001b[39m\u001b[39m\"\u001b[39m, type_\u001b[39m=\u001b[39msqltypes\u001b[39m.\u001b[39mUnicode))\n\u001b[0;32m-> 3736\u001b[0m c \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mexecute(s, \u001b[39mdict\u001b[39;49m(table_name\u001b[39m=\u001b[39;49mtable_name, schema\u001b[39m=\u001b[39;49mschema))\n\u001b[1;32m   3737\u001b[0m table_oid \u001b[39m=\u001b[39m c\u001b[39m.\u001b[39mscalar()\n\u001b[1;32m   3738\u001b[0m \u001b[39mif\u001b[39;00m table_oid \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1380\u001b[0m, in \u001b[0;36mConnection.execute\u001b[0;34m(self, statement, *multiparams, **params)\u001b[0m\n\u001b[1;32m   1376\u001b[0m     util\u001b[39m.\u001b[39mraise_(\n\u001b[1;32m   1377\u001b[0m         exc\u001b[39m.\u001b[39mObjectNotExecutableError(statement), replace_context\u001b[39m=\u001b[39merr\n\u001b[1;32m   1378\u001b[0m     )\n\u001b[1;32m   1379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1380\u001b[0m     \u001b[39mreturn\u001b[39;00m meth(\u001b[39mself\u001b[39;49m, multiparams, params, _EMPTY_EXECUTION_OPTS)\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/lib/python3.10/site-packages/sqlalchemy/sql/elements.py:334\u001b[0m, in \u001b[0;36mClauseElement._execute_on_connection\u001b[0;34m(self, connection, multiparams, params, execution_options, _force)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_execute_on_connection\u001b[39m(\n\u001b[1;32m    331\u001b[0m     \u001b[39mself\u001b[39m, connection, multiparams, params, execution_options, _force\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    332\u001b[0m ):\n\u001b[1;32m    333\u001b[0m     \u001b[39mif\u001b[39;00m _force \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_execution:\n\u001b[0;32m--> 334\u001b[0m         \u001b[39mreturn\u001b[39;00m connection\u001b[39m.\u001b[39;49m_execute_clauseelement(\n\u001b[1;32m    335\u001b[0m             \u001b[39mself\u001b[39;49m, multiparams, params, execution_options\n\u001b[1;32m    336\u001b[0m         )\n\u001b[1;32m    337\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m         \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mObjectNotExecutableError(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1572\u001b[0m, in \u001b[0;36mConnection._execute_clauseelement\u001b[0;34m(self, elem, multiparams, params, execution_options)\u001b[0m\n\u001b[1;32m   1560\u001b[0m compiled_cache \u001b[39m=\u001b[39m execution_options\u001b[39m.\u001b[39mget(\n\u001b[1;32m   1561\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcompiled_cache\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine\u001b[39m.\u001b[39m_compiled_cache\n\u001b[1;32m   1562\u001b[0m )\n\u001b[1;32m   1564\u001b[0m compiled_sql, extracted_params, cache_hit \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_compile_w_cache(\n\u001b[1;32m   1565\u001b[0m     dialect\u001b[39m=\u001b[39mdialect,\n\u001b[1;32m   1566\u001b[0m     compiled_cache\u001b[39m=\u001b[39mcompiled_cache,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1570\u001b[0m     linting\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdialect\u001b[39m.\u001b[39mcompiler_linting \u001b[39m|\u001b[39m compiler\u001b[39m.\u001b[39mWARN_LINTING,\n\u001b[1;32m   1571\u001b[0m )\n\u001b[0;32m-> 1572\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_context(\n\u001b[1;32m   1573\u001b[0m     dialect,\n\u001b[1;32m   1574\u001b[0m     dialect\u001b[39m.\u001b[39;49mexecution_ctx_cls\u001b[39m.\u001b[39;49m_init_compiled,\n\u001b[1;32m   1575\u001b[0m     compiled_sql,\n\u001b[1;32m   1576\u001b[0m     distilled_params,\n\u001b[1;32m   1577\u001b[0m     execution_options,\n\u001b[1;32m   1578\u001b[0m     compiled_sql,\n\u001b[1;32m   1579\u001b[0m     distilled_params,\n\u001b[1;32m   1580\u001b[0m     elem,\n\u001b[1;32m   1581\u001b[0m     extracted_params,\n\u001b[1;32m   1582\u001b[0m     cache_hit\u001b[39m=\u001b[39;49mcache_hit,\n\u001b[1;32m   1583\u001b[0m )\n\u001b[1;32m   1584\u001b[0m \u001b[39mif\u001b[39;00m has_events:\n\u001b[1;32m   1585\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mafter_execute(\n\u001b[1;32m   1586\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m   1587\u001b[0m         elem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1591\u001b[0m         ret,\n\u001b[1;32m   1592\u001b[0m     )\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1943\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1940\u001b[0m             branched\u001b[39m.\u001b[39mclose()\n\u001b[1;32m   1942\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 1943\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_dbapi_exception(\n\u001b[1;32m   1944\u001b[0m         e, statement, parameters, cursor, context\n\u001b[1;32m   1945\u001b[0m     )\n\u001b[1;32m   1947\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/lib/python3.10/site-packages/sqlalchemy/engine/base.py:2128\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[1;32m   2124\u001b[0m         util\u001b[39m.\u001b[39mraise_(\n\u001b[1;32m   2125\u001b[0m             sqlalchemy_exception, with_traceback\u001b[39m=\u001b[39mexc_info[\u001b[39m2\u001b[39m], from_\u001b[39m=\u001b[39me\n\u001b[1;32m   2126\u001b[0m         )\n\u001b[1;32m   2127\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2128\u001b[0m         util\u001b[39m.\u001b[39;49mraise_(exc_info[\u001b[39m1\u001b[39;49m], with_traceback\u001b[39m=\u001b[39;49mexc_info[\u001b[39m2\u001b[39;49m])\n\u001b[1;32m   2130\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   2131\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reentrant_error\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/lib/python3.10/site-packages/sqlalchemy/util/compat.py:211\u001b[0m, in \u001b[0;36mraise_\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    208\u001b[0m     exception\u001b[39m.\u001b[39m__cause__ \u001b[39m=\u001b[39m replace_context\n\u001b[1;32m    210\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     \u001b[39mraise\u001b[39;00m exception\n\u001b[1;32m    212\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# credit to\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# as the __traceback__ object creates a cycle\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39mdel\u001b[39;00m exception, replace_context, from_, with_traceback\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1900\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1898\u001b[0m                 \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   1899\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m evt_handled:\n\u001b[0;32m-> 1900\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdialect\u001b[39m.\u001b[39;49mdo_execute(\n\u001b[1;32m   1901\u001b[0m             cursor, statement, parameters, context\n\u001b[1;32m   1902\u001b[0m         )\n\u001b[1;32m   1904\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_events \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine\u001b[39m.\u001b[39m_has_events:\n\u001b[1;32m   1905\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mafter_cursor_execute(\n\u001b[1;32m   1906\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m   1907\u001b[0m         cursor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1911\u001b[0m         context\u001b[39m.\u001b[39mexecutemany,\n\u001b[1;32m   1912\u001b[0m     )\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/lib/python3.10/site-packages/sqlalchemy/engine/default.py:736\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_execute\u001b[39m(\u001b[39mself\u001b[39m, cursor, statement, parameters, context\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 736\u001b[0m     cursor\u001b[39m.\u001b[39;49mexecute(statement, parameters)\n",
      "File \u001b[0;32m/scratch/gpfs/HSC/LSST/stack_20230302/conda/envs/lsst-scipipe-4.0.1/lib/python3.10/encodings/utf_8.py:15\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(input, errors)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m### Codec APIs\u001b[39;00m\n\u001b[1;32m     13\u001b[0m encode \u001b[39m=\u001b[39m codecs\u001b[39m.\u001b[39mutf_8_encode\n\u001b[0;32m---> 15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39minput\u001b[39m, errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39mutf_8_decode(\u001b[39minput\u001b[39m, errors, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mIncrementalEncoder\u001b[39;00m(codecs\u001b[39m.\u001b[39mIncrementalEncoder):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for tract in tracts_n540_nogaap[:5]:\n",
    "    deploy_training_job(tract, band = \"N540\", filter_jobs=5,\n",
    "                        python_file='lambo/scripts/hsc_gaap/run_gaap.py',\n",
    "                        name='gaap', email=\"am2907@princeton.edu\", outname = None, \n",
    "                        repo='/scratch/gpfs/am2907/Merian/gaap', scriptdir=\"/scratch/gpfs/am2907/Merian/gaap/\", \n",
    "                        submit=True, fixpatches=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gaap reduction will save one catalog for each patch to (for example):\n",
    "\n",
    "        /scratch/gpfs/am2907/Merian/gaap/S20A/gaapTable/9813/0,0/objectTable_9813_0,0_S20A.fits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 4: Check on it!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check on the logs while the jobs are running to check for any glaring problems:\n",
    "- `logs/gaapPhot_array_9813_0.o` \n",
    "- `logs/gaap_9813_0.log`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One the jobs are done running (for a given tract), you can check how things went. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9828, 9833, 9837, 9838, 9839, 9862, 9863, 9939, 9940, 9941, 9942, 9943, 9944, 9945, 9949, 9950, 9951, 9952, 9953, 10040, 10041, 10042, 10043, 10044, 10045, 10046, 10047, 10048, 10049, 10050, 10051, 10052, 10053, 10057, 10058, 10060, 10061, 10062, 10070, 10078, 10182, 10183, 10184, 10185, 10186, 10283, 10284, 10285, 10286, 10287, 10288, 10289, 10290, 10291, 10292, 10293, 10294, 10295, 10296, 10297, 10298, 10299, 10300, 10301, 10302, 10303, 10304, 10426, 10427, 10428]\n"
     ]
    }
   ],
   "source": [
    "print(tracts_notcompiled_N708[40:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRACT: 9618\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9619\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9620\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9621\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9697\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9698\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9699\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9700\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9701\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9702\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9703\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9707\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9708\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9709\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9710\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9711\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9712\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9713\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9714\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9798\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9799\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9800\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9801\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9802\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9803\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9804\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9805\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9806\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9807\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9808\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9809\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9810\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9811\n",
      "NO PROBLEMS\n",
      "\n",
      "TRACT: 9815\n",
      "MISSING PATCHES\n",
      "\n",
      "TRACT: 9816\n",
      "MISSING PATCHES\n",
      "\n",
      "TRACT: 9817\n",
      "MISSING PATCHES\n",
      "\n",
      "TRACT: 9818\n",
      "MISSING PATCHES\n",
      "\n",
      "TRACT: 9819\n",
      "MISSING PATCHES\n",
      "\n",
      "TRACT: 9820\n",
      "MISSING PATCHES\n",
      "\n",
      "TRACT: 9821\n",
      "MISSING PATCHES\n",
      "\n",
      "TRACT: 9828\n",
      "MISSING PATCHES\n",
      "\n",
      "TRACT: 9833\n",
      "MISSING PATCHES\n",
      "\n",
      "TRACT: 9837\n",
      "MISSING PATCHES\n",
      "\n",
      "TRACT: 9838\n",
      "MISSING PATCHES\n",
      "\n",
      "TRACT: 9839\n",
      "MISSING PATCHES\n",
      "\n",
      "TRACT: 9862\n",
      "MISSING PATCHES\n",
      "\n",
      "TRACT: 9863\n",
      "MISSING PATCHES\n",
      "\n",
      "TRACT: 9939\n",
      "MISSING PATCHES\n",
      "\n",
      "TRACT: 9940\n",
      "MISSING PATCHES\n",
      "\n",
      "TRACT: 9941\n",
      "MISSING PATCHES\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tract in tracts_notcompiled_N708[0:50]:\n",
    "    problems = checkRun(tract, band=\"N708\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRACT: 9812\n",
      "NO PROBLEMS\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkRun(9812)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might get issues like \"Failed for 3 bands\" - this could be because HSC images don't exist for all bands. So it might not be an issue you can fix!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 4: Merge catalogs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is looking good, you can merge the patch catalogs into a tract-level catalog. \n",
    "\n",
    "It's recommended to run this step in a screen in terminal, because it takes some time!\n",
    "\n",
    "But here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPILING CATALOG FOR TRACT 9617 WITH 13 PATCHES\n",
      "COMPILED TABLE OF 279484 ROWS and 69 COLUMNS\n",
      "WROTE TABLE TO /scratch/gpfs/am2907/Merian/gaap/S20A/gaapTable/9617/objectTable_9617_S20A.fits\n"
     ]
    }
   ],
   "source": [
    "compileCatalogs([9617], repo, alltracts=False, rewrite=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        python3 lambo/scripts/hsc_gaap/compile_catalogs.py --tracts==\"[9327,9328,9329,9813,9812]\"\n",
    "\n",
    "This will save a catalog to (for example):\n",
    "        \n",
    "        /scratch/gpfs/am2907/Merian/gaap/S20A/gaapTable/9813/objectTable_9813_S20A.fits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to change the columns that are used for the compiled catalog, edit these files:\n",
    "\n",
    "        lambo/scripts/hsc_gaap/keep_table_columns_gaap.txt\n",
    "        lambo/scripts/hsc_gaap/keep_table_columns_merian.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you're all done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
